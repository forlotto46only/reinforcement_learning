{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98597d6-eece-4b6a-acf9-48df3faa7fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 상태 가치 테이블:\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "\n",
      "최종 정책(각 상태에서의 행동 확률):\n",
      "상태 (0,0): [0. 0. 0. 0.]\n",
      "상태 (0,1): [0. 0. 1. 0.]\n",
      "상태 (0,2): [0. 0. 1. 0.]\n",
      "상태 (0,3): [0.  0.5 0.5 0. ]\n",
      "상태 (1,0): [1. 0. 0. 0.]\n",
      "상태 (1,1): [0. 0. 1. 0.]\n",
      "상태 (1,2): [0.  0.5 0.5 0. ]\n",
      "상태 (1,3): [0. 1. 0. 0.]\n",
      "상태 (2,0): [1. 0. 0. 0.]\n",
      "상태 (2,1): [0.5 0.  0.  0.5]\n",
      "상태 (2,2): [0. 0. 0. 1.]\n",
      "상태 (2,3): [0. 1. 0. 0.]\n",
      "상태 (3,0): [0.5 0.  0.  0.5]\n",
      "상태 (3,1): [0. 0. 0. 1.]\n",
      "상태 (3,2): [0. 0. 0. 1.]\n",
      "상태 (3,3): [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# (1) 그리드월드 환경 설정 (4x4)\n",
    "grid_size = 4\n",
    "gamma = 1.0  # 감가율\n",
    "\n",
    "# (2) 상태 가치 초기화\n",
    "value_table = np.zeros((grid_size, grid_size))\n",
    "\n",
    "# (3) 종료 상태(목적지)\n",
    "terminal_states = [(0, 0), (grid_size - 1, grid_size - 1)]\n",
    "\n",
    "# (4) 가능한 행동: 상, 하, 좌, 우\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "\n",
    "# (5) 상태 전이 확률(각 방향 25%)\n",
    "action_prob = 0.25\n",
    "\n",
    "# (6) 정책 평가 함수 정의\n",
    "def policy_evaluation(value_table, iterations=100):\n",
    "    for _ in range(iterations):\n",
    "        new_value_table = np.copy(value_table)\n",
    "\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                if (i, j) in terminal_states:\n",
    "                    continue\n",
    "                value = 0\n",
    "\n",
    "                for action in actions:\n",
    "                    next_i, next_j = i + action[0], j + action[1]\n",
    "\n",
    "                    # 경계를 벗어나면 현재 위치 유지\n",
    "                    if next_i < 0 or next_i >= grid_size or next_j < 0 or next_j >= grid_size:\n",
    "                        next_i, next_j = i, j\n",
    "\n",
    "                    reward = -1\n",
    "                    value += action_prob * (reward + gamma * value_table[next_i, next_j])\n",
    "\n",
    "                new_value_table[i, j] = value\n",
    "\n",
    "        value_table = new_value_table\n",
    "\n",
    "    return value_table\n",
    "\n",
    "# (7) 정책 개선 함수 정의\n",
    "def policy_improvement(value_table):\n",
    "    policy = np.zeros((grid_size, grid_size, len(actions)))\n",
    "\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            if (i, j) in terminal_states:\n",
    "                continue\n",
    "\n",
    "            values = []\n",
    "            for action in actions:\n",
    "                next_i, next_j = i + action[0], j + action[1]\n",
    "\n",
    "                if next_i < 0 or next_i >= grid_size or next_j < 0 or next_j >= grid_size:\n",
    "                    next_i, next_j = i, j\n",
    "\n",
    "                values.append(value_table[next_i, next_j])\n",
    "\n",
    "            best_action_value = np.max(values)\n",
    "\n",
    "            # 가장 높은 가치로 이동할 수 있는 행동 찾기\n",
    "            best_actions = [idx for idx, v in enumerate(values) if v == best_action_value]\n",
    "            \n",
    "            # 탐욕적(Greedy) 행동 선택\n",
    "            for action_idx in best_actions:\n",
    "                policy[i, j, action_idx] = 1 / len(best_actions)\n",
    "\n",
    "    return policy\n",
    "\n",
    "# (8) 정책 반복(평가 및 개선 반복 수행)\n",
    "def policy_iteration(iterations=10):\n",
    "    global value_table\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        value_table = policy_evaluation(value_table)\n",
    "        policy = policy_improvement(value_table)\n",
    "\n",
    "    return value_table, policy\n",
    "\n",
    "# 실행 및 결과 출력\n",
    "final_values, final_policy = policy_iteration()\n",
    "\n",
    "# (9) 최종 상태 가치 테이블\n",
    "print(\"최종 상태 가치 테이블:\")\n",
    "print(np.round(final_values, 2))\n",
    "\n",
    "# (10) 최정 정책\n",
    "print(\"\\n최종 정책(각 상태에서의 행동 확률):\")\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        print(f\"상태 ({i},{j}): {final_policy[i,j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c82b9-a1a4-4d1e-bae6-5d41e2c58a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
