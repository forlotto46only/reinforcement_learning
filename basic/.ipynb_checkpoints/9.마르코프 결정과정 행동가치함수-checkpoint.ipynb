{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c640df8a-bc9d-4d85-9089-2a08aa1372d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 상태가치함수 Vπ(s):\n",
      "  S: 2.4113\n",
      "  R1: 2.7236\n",
      "  R2: 3.5000\n",
      "  R3: 3.0000\n",
      "  F: 0.0000\n",
      "\n",
      "✅ 행동가치함수 Qπ(s,a):\n",
      "  (S, A1): 1.8618\n",
      "  (S, A2): 3.2500\n",
      "  (R1, A1): 2.5000\n",
      "  (R1, A2): 3.2500\n",
      "  (R2, A1): 3.5000\n",
      "  (R3, A1): 3.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# (1) 상태와 행동 정의\n",
    "states = [\"S\", \"R1\", \"R2\", \"R3\", \"F\"]\n",
    "actions = [\"A1\", \"A2\"]\n",
    "gamma = 0.5  # 감가율\n",
    "\n",
    "# (2) 정책 π(s, a): 상태에서 행동을 선택할 확률\n",
    "policy = {\n",
    "    \"S\": {\"A1\": 0.6, \"A2\": 0.4},\n",
    "    \"R1\": {\"A1\": 0.7, \"A2\": 0.3},\n",
    "    \"R2\": {\"A1\": 1.0},\n",
    "    \"R3\": {\"A1\": 1.0}\n",
    "}\n",
    "\n",
    "# (3) 상태 전이 확률 P(s, a, s')\n",
    "transition_probs = {\n",
    "    (\"S\", \"A1\"): \"R1\",\n",
    "    (\"S\", \"A2\"): \"R2\",\n",
    "    (\"R1\", \"A1\"): \"R3\",\n",
    "    (\"R1\", \"A2\"): \"R2\",\n",
    "    (\"R2\", \"A1\"): \"R3\",\n",
    "    (\"R3\", \"A1\"): \"F\"\n",
    "}\n",
    "\n",
    "# (4) 보상 함수 R(s, a)\n",
    "rewards = {\n",
    "    (\"S\", \"A1\"): 0.5,\n",
    "    (\"S\", \"A2\"): 1.5,\n",
    "    (\"R1\", \"A1\"): 1.0,\n",
    "    (\"R1\", \"A2\"): 1.5,\n",
    "    (\"R2\", \"A1\"): 2.0,\n",
    "    (\"R3\", \"A1\"): 3.0\n",
    "}\n",
    "\n",
    "# (5) 에피소드 시뮬레이션 함수 (정책 기반 경로)\n",
    "def simulate_episode(start_state=\"S\"):\n",
    "    state = start_state\n",
    "    total_return = 0\n",
    "    discount = 1.0\n",
    "\n",
    "    while state != \"F\":\n",
    "        action_probs = policy[state]\n",
    "        actions_list = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        action = np.random.choice(actions_list, p=probs)\n",
    "\n",
    "        reward = rewards.get((state, action), 0)\n",
    "        total_return += discount * reward\n",
    "\n",
    "        next_state = transition_probs.get((state, action), \"F\")\n",
    "        state = next_state\n",
    "        discount *= gamma\n",
    "\n",
    "    return total_return\n",
    "\n",
    "n_episodes = 10000\n",
    "state_values = {}\n",
    "q_values = {}\n",
    "\n",
    "# (6) 상태가치함수 계산\n",
    "for state in states:\n",
    "    episode_returns = []  # 각 상태에서 시작한 에피소드들의 보상 저장 리스트\n",
    "\n",
    "    # 여러 번 시뮬레이션 실행\n",
    "    for _ in range(n_episodes):\n",
    "        result = simulate_episode(start_state=state)  # 해당 상태에서 에피소드 시작\n",
    "        episode_returns.append(result)  # 반환값 저장\n",
    "\n",
    "    # 평균 보상을 상태가치로 저장\n",
    "    state_values[state] = np.mean(episode_returns)\n",
    "\n",
    "# (7) 행동가치함수 계산\n",
    "for state in policy:\n",
    "    for action in policy[state]:\n",
    "        returns = []\n",
    "        for _ in range(n_episodes):\n",
    "            temp_state = state\n",
    "            total_return = 0\n",
    "            discount = 1.0\n",
    "\n",
    "            #(7-1) 첫 행동을 강제로 선택\n",
    "            reward = rewards.get((temp_state, action), 0)\n",
    "            total_return += discount * reward\n",
    "            next_state = transition_probs.get((temp_state, action), \"F\")\n",
    "            temp_state = next_state\n",
    "            discount *= gamma\n",
    "\n",
    "            #(7-2) 이후부터는 정책에 따라 행동\n",
    "            while temp_state != \"F\":\n",
    "                action_probs = policy[temp_state]\n",
    "                actions_list = list(action_probs.keys())\n",
    "                probs = list(action_probs.values())\n",
    "                next_action = np.random.choice(actions_list, p=probs)\n",
    "\n",
    "                reward = rewards.get((temp_state, next_action), 0)\n",
    "                total_return += discount * reward\n",
    "                temp_state = transition_probs.get((temp_state, next_action), \"F\")\n",
    "                discount *= gamma\n",
    "\n",
    "            returns.append(total_return)\n",
    "        q_values[(state, action)] = np.mean(returns)\n",
    "\n",
    "# (8) 결과 출력\n",
    "print(\"\\n✅ 상태가치함수 Vπ(s):\")\n",
    "for s, v in state_values.items():\n",
    "    print(f\"  {s}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n✅ 행동가치함수 Qπ(s,a):\")\n",
    "for (s, a), q in q_values.items():\n",
    "    print(f\"  ({s}, {a}): {q:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d9f1d-7b94-4a17-b522-fc6854d4bbb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301545d-e784-4d43-a1d4-eb10e426eb55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
